{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gross-oakland",
   "metadata": {},
   "source": [
    "# Accurity Glossary Mapping Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "applied-designer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sql\n",
    "import requests\n",
    "import textdistance as td\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import accurity.nbconfig\n",
    "# from accurity.sql.adventure_works import s_tables, s_columns\n",
    "from accurity.tfidfmapping import TfidfDistanceMapping\n",
    "from accurity.textmapping import TextDistanceMapping\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-template",
   "metadata": {},
   "source": [
    "## 1. Adventure Works (AW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sql.create_engine(os.environ[\"ADVENTURE_WORKS_DB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = (\n",
    "    pd.read_sql(s_tables, engine)\n",
    "    .rename(columns={\"TABLE_SCHEMA\": \"table_schema\", \"TABLE_NAME\": \"table_name\"})\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "columns = (\n",
    "    pd.read_sql(s_columns, engine)\n",
    "    .rename(columns={\"TABLE_SCHEMA\": \"schema\", \"TABLE_NAME\": \"table\",\n",
    "            \"COLUMN_NAME\": \"column\",\"IS_NULLABLE\": \"is_nullable\",\n",
    "            \"DATA_TYPE\": \"type\"})\n",
    "    .assign(table_column=lambda x: x.table + \".\" + x.column)\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(by=[\"schema\", \"table\", \"column\"])\n",
    ")\n",
    "columns = columns[~columns.schema.str.startswith(\"dbo\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_terms = pd.read_csv(\"./data/AW/business_terms.csv\")\n",
    "business_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "jw_dist_candidates = find_mapping_candidates(sequences=columns.table_column,candidates=business_terms.business_term,\n",
    "                                             distance_func=jaro_winkler, reverse=-1,\n",
    "                                             N=5, return_type=\"df\", sequence_name=\"column\", candidate_name=\"bt\",\n",
    "                                             fpath='./data/mapping_candidates.xlsx')\n",
    "jw_dist_candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_dist_candidates = find_mapping_candidates(sequences=columns.table_column,\n",
    "                                              candidates=business_terms.business_term, distance_func=levenshtein,\n",
    "                                              N=5, return_type=\"df\", sequence_name=\"column\", candidate_name=\"bt\",\n",
    "                                              fpath='./data/mapping_candidates.xlsx')\n",
    "lev_dist_candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlev_dist_candidates = find_mapping_candidates(sequences=columns.table_column,candidates=business_terms.business_term,\n",
    "                                              distance_func=damerau_levenshtein, reverse=1,\n",
    "                                              N=5, return_type=\"excel\", sequence_name=\"column\", candidate_name=\"bt\",\n",
    "                                              fpath='./data/mapping_candidates.xlsx')\n",
    "dlev_dist_candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlev_dist_candidates = find_mapping_candidates(sequences=columns['table_column'], candidates=business_terms.business_term,\n",
    "                                              distance_func=needleman_wunsch, reverse=-1,\n",
    "                                              N=5, return_type=\"excel\", sequence_name=\"column\", candidate_name=\"bt\",\n",
    "                                              fpath='./data/mapping_candidates.xlsx')\n",
    "dlev_dist_candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_algorithms = [td.jaro_winkler, td.jaro, td.levenshtein, td.damerau_levenshtein]\n",
    "for algo in edit_algorithms:\n",
    "    test.find_text_mapping(algorithm=algo, ngrams=1)\n",
    "    test.save_text_mapping_to_xls(algorithm=algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-poultry",
   "metadata": {},
   "source": [
    "## 2. Groupe Société Générale (BRD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-landscape",
   "metadata": {},
   "source": [
    "### 2.1 Text-Distance Mapping Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-coverage",
   "metadata": {},
   "source": [
    "#### 2.1.1 Load data for BRD object (`ncandidates=5`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-hacker",
   "metadata": {},
   "source": [
    "Initialise new or load from binary file stored `BusinessTermsMapping` object for BRD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "divine-story",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextDistanceMappings(name=BRD, validation=True, model_file=../models/text_mapping_brd.bin, raw_data_dir=../data/raw/BRD, output_data_dir=../data/output/, ncandidates=5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brd = TextDistanceMapping(name='BRD', raw_data_dir='../data/raw/BRD', validation=True)\n",
    "brd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-genius",
   "metadata": {},
   "source": [
    "Refresh loading input data for running the mapping algorithm from files `data_fields.csv`, `business_terms.csv`, as well as validation data for validating identifed mappings from file `data_mappings.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "technical-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "brd.initialize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-walnut",
   "metadata": {},
   "source": [
    "#### 2.1.2 Finding mapping candidates using edit-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-malta",
   "metadata": {},
   "source": [
    "Run `edit-based distance` algorithms to find best business terms candidates for each data field. Then, save identified mapping candidates into respective excel sheet and validate the mapping candidates versus the available validation mappings prepared by the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_algorithms = [td.jaro_winkler, td.jaro, td.levenshtein, td.damerau_levenshtein]\n",
    "for algo in edit_algorithms:\n",
    "    brd.find_save_mapping(algorithm=algo, ngrams=1)\n",
    "    \n",
    "brd.find_save_mapping(algorithm=td.jaro_winkler, ngrams=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-friday",
   "metadata": {},
   "source": [
    "#### 2.1.3 Finding mapping candidates using token-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-cabinet",
   "metadata": {},
   "source": [
    "Run `token-based distance` algorithms to find best business terms candidates for each data field. Then, save identified mapping candidates into respective excel sheet and validate the mapping candidates versus the available validation mappings prepared by the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_algorithms = [td.overlap, td.tversky, td.sorensen, td.cosine, td.jaccard]\n",
    "for algo in token_algorithms:\n",
    "    brd.find_save_mapping(algorithm=algo, ngrams=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-proof",
   "metadata": {},
   "source": [
    "#### 2.1.4 Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "brd.print_validation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-blood",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF Distance Mapping Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-travel",
   "metadata": {},
   "source": [
    "#### 2.2.1 Load data for BRD object (`ncandidates=5`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brd_tfidf = TfidfDistanceMapping(name='BRD', raw_data_dir='../data/raw/BRD', validation=True)\n",
    "brd_tfidf = TfidfDistanceMapping.load_model(model_file='../models/tfidf_mapping_brd.bin')\n",
    "brd_tfidf.initialize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-advocacy",
   "metadata": {},
   "source": [
    "#### 2.2.2 Finding mapping candidates using cosine similarity and vectorization algorithm based on TF-IDF weighting and different tokenization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-external",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brd_tfidf.find_save_mapping(ngrams=3, analyzer='char', use_stemmer=True)\n",
    "brd_tfidf.find_save_mapping(ngrams=3, analyzer='char_wb', use_stemmer=True)\n",
    "brd_tfidf.find_save_mapping(ngrams=1, analyzer='word', use_stemmer=True)\n",
    "brd_tfidf.find_save_mapping(ngrams=3, analyzer='word', use_stemmer=True)\n",
    "\n",
    "brd_tfidf.find_save_mapping(ngrams=3, analyzer='char', use_stemmer=False)\n",
    "brd_tfidf.find_save_mapping(ngrams=3, analyzer='char_wb', use_stemmer=False)\n",
    "brd_tfidf.find_save_mapping(ngrams=1, analyzer='word', use_stemmer=False)\n",
    "brd_tfidf.find_save_mapping(ngrams=3, analyzer='word', use_stemmer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-theory",
   "metadata": {},
   "source": [
    "#### 2.1.3 Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-avatar",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brd_tfidf.print_validation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-vertical",
   "metadata": {},
   "source": [
    "## 3. Bank of Ireland (BOI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-server",
   "metadata": {},
   "source": [
    "### 3.1 Text-Distance Mapping Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-signature",
   "metadata": {},
   "source": [
    "#### 3.1.1 Load data for BOI object (`ncandidates=5`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-ghana",
   "metadata": {},
   "source": [
    "Initialise new or load from binary file stored `BusinessTermsMapping` object for BOI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boi = TextDistanceMapping(name='BOI', raw_data_dir='../data/raw/BOI', validation=True, sep='; ')\n",
    "boi = TextDistanceMapping.load_model(model_file='../models/text_mapping_boi.bin')\n",
    "boi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-albuquerque",
   "metadata": {},
   "source": [
    "Refresh loading input data for running the mapping algorithm from files `data_fields.csv`, `business_terms.csv`, as well as validation data for validating identifed mappings from file `data_mappings.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "boi.initialize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-sussex",
   "metadata": {},
   "source": [
    "#### 3.1.2 Finding mapping candidates using edit-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-duration",
   "metadata": {},
   "source": [
    "Run `edit-based distance` algorithms to find best business terms candidates for each data field. Then, save identified mapping candidates into respective excel sheet and validate the mapping candidates versus the available validation mappings prepared by the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit_algorithms = [td.jaro_winkler, td.jaro, td.levenshtein, td.damerau_levenshtein]\n",
    "# for algo in edit_algorithms:\n",
    "#     boi.find_save_mapping(algorithm=algo, ngrams=1)\n",
    "    \n",
    "boi.find_save_mapping(algorithm=td.jaro_winkler, ngrams=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-mexico",
   "metadata": {},
   "source": [
    "#### 3.1.3 Finding mapping candidates using token-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-endorsement",
   "metadata": {},
   "source": [
    "Run `token-based distance` algorithms to find best business terms candidates for each data field. Then, save identified mapping candidates into respective excel sheet and validate the mapping candidates versus the available validation mappings prepared by the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_algorithms = [td.overlap, td.tversky, td.sorensen, td.cosine, td.jaccard]\n",
    "for algo in token_algorithms:\n",
    "    boi.find_save_mapping(algorithm=algo, ngrams=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-start",
   "metadata": {},
   "source": [
    "#### 3.1.4 Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "boi.print_validation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-interval",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF Distance Mapping Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-token",
   "metadata": {},
   "source": [
    "#### 3.2.1 Load data for BOI object (`ncandidates=5`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boi_tfidf = TfidfDistanceMapping(name='BOI', raw_data_dir='../data/raw/BOI', validation=True, sep='; ')\n",
    "boi_tfidf = TfidfDistanceMapping.load_model(model_file='../models/tfidf_mapping_boi.bin')\n",
    "boi_tfidf.initialize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-minority",
   "metadata": {},
   "source": [
    "#### 3.2.2 Finding mapping candidates using cosine similarity and vectorization algorithm based on TF-IDF weighting and different tokenization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "boi_tfidf.find_save_mapping(ngrams=3, analyzer='char', use_stemmer=True)\n",
    "boi_tfidf.find_save_mapping(ngrams=3, analyzer='char_wb', use_stemmer=True)\n",
    "boi_tfidf.find_save_mapping(ngrams=1, analyzer='word', use_stemmer=True)\n",
    "boi_tfidf.find_save_mapping(ngrams=3, analyzer='word', use_stemmer=True)\n",
    "\n",
    "boi_tfidf.find_save_mapping(ngrams=3, analyzer='char', use_stemmer=False)\n",
    "boi_tfidf.find_save_mapping(ngrams=3, analyzer='char_wb', use_stemmer=False)\n",
    "boi_tfidf.find_save_mapping(ngrams=1, analyzer='word', use_stemmer=False)\n",
    "boi_tfidf.find_save_mapping(ngrams=3, analyzer='word', use_stemmer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-difficulty",
   "metadata": {},
   "source": [
    "#### 3.2.3 Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "boi_tfidf.print_validation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://http.cat/100\")\n",
    "\n",
    "print(response.headers.get(\"Content-Type\"))\n",
    "with open(\"goat.jpeg\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "display(Image(filename='goat.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlify(in_string):\n",
    "    return in_string.replace(' ', '%20')\n",
    "\n",
    "base_url = \"http://127.0.0.1:8001/api/1.0/mappings\"\n",
    "data_field = 'account appl rel type.account appl rel type cd'\n",
    "\n",
    "people_endpoint = os.path.join(base_url, urlify(data_field))\n",
    "headers = {'X-API-Key': 'btmappings'}\n",
    "\n",
    "r = requests.get(url=people_endpoint, headers=headers)\n",
    "r.json()[data_field]\n",
    "\n",
    "\n",
    "# r = requests.get(url=people_endpoint, headers=headers)\n",
    "# print(r.status_code, r.reason, r.json()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 3, 2, 1]\n",
    "\n",
    "def longestPeak(a):\n",
    "    maxpeak = 0\n",
    "    peak = 0\n",
    "    asc, desc = False, False\n",
    "    \n",
    "    for i in range(2, len(a)):\n",
    "        print(f\"Befor i:{i:>2} => {a[i]:>2}, peak:{peak}, maxpeak:{maxpeak}\")\n",
    "        if a[i-2] < a[i-1] < a[i]:\n",
    "            if i == 2:\n",
    "                peak += 2\n",
    "            else:\n",
    "                peak += 1\n",
    "            asc = True\n",
    "        if a[i-2] > a[i-1] < a[i]:\n",
    "            if peak + 1 > maxpeak and asc and desc:\n",
    "                maxpeak = peak + 1\n",
    "            peak = 1\n",
    "            asc = True\n",
    "        if a[i-2] > a[i-1] == a[i]:\n",
    "            if peak + 1 > maxpeak and asc and desc:\n",
    "                maxpeak = peak + 1\n",
    "            peak = 0\n",
    "            asc = 0\n",
    "            desc = 0\n",
    "        if a[i-2] > a[i-1] > a[i]:\n",
    "            peak += 1\n",
    "            desc = True\n",
    "            if peak + 1 > maxpeak and asc and desc:\n",
    "                maxpeak = peak + 1\n",
    "        if a[i-2] < a[i-1] > a[i]:\n",
    "            if i == 2:\n",
    "                peak += 2\n",
    "            else:\n",
    "                peak += 1\n",
    "            desc = True\n",
    "            asc = True\n",
    "            if peak + 1 > maxpeak and asc and desc:\n",
    "                maxpeak = peak + 1\n",
    "        if a[i-1] == a[i]:\n",
    "            if peak + 1 > maxpeak and asc and desc:\n",
    "                maxpeak = peak + 1\n",
    "            peak = 0\n",
    "            asc = 0\n",
    "            desc = 0\n",
    "        if a[i-2] == a[i-1] < a[i]:\n",
    "            peak += 1\n",
    "            asc = True\n",
    "        print(f\"After i:{i:>2} => {a[i]:>2}, peak:{peak}, maxpeak:{maxpeak}\\n\")\n",
    "    return maxpeak\n",
    "\n",
    "longestPeak(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [[1, 2], [3, 5], [4, 7], [6, 8], [9, 10]]\n",
    "intervals = [[89, 90], [-10, 20], [-50, 0], [70, 90], [90, 91], [90, 95]]\n",
    "intervals = [[1, 22], [-20, 30]]\n",
    "\n",
    "def merge_intervals(intervals):\n",
    "    intervals.sort(key=lambda x: (x[0], x[1]))\n",
    "    print(intervals)\n",
    "    \n",
    "    result = []\n",
    "    current = intervals[0]\n",
    "    \n",
    "    i = 1    \n",
    "    while i < len(intervals):\n",
    "        if current[1] >= intervals[i][0]:\n",
    "            current[1] = max(intervals[i][1], current[1])\n",
    "        else:\n",
    "            result.append(current)\n",
    "            current = intervals[i]\n",
    "        if i == len(intervals) - 1:\n",
    "            result.append(current)\n",
    "        print(i, current)\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "merge_intervals(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1, 2, 4, 7, 10, 11, 7, 12, 6, 7, 16, 18, 19]\n",
    "\n",
    "def subarraysort(array):\n",
    "    sorted_array = sorted(array)\n",
    "    for i in range(len(array)):\n",
    "        if array[i] != sorted_array[i]:\n",
    "            start = i\n",
    "            break\n",
    "    for i in reversed(range(len(array))):\n",
    "        if array[i] != sorted_array[i]:\n",
    "            end = i\n",
    "            break\n",
    "    return start, end\n",
    "\n",
    "subarraysort(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1, 11, 3, 0, 15, 5, 2, 4, 10, 7, 12, 6]\n",
    "array = [1, 2, 3, 4]\n",
    "array = [19, -1, 18, 17, 2, 10, 3, 12, 5, 16, 4, 11, 8, 7, 6, 15, 12, 12, 2, 1, 6, 13, 14]\n",
    "\n",
    "def largestRange(array):\n",
    "    array.sort()\n",
    "    maxstart, maxend = array[0], array[0]\n",
    "    start, end = maxstart, maxend\n",
    "    for i in range(1, len(array)):\n",
    "        if array[i] - array[i-1] <= 1:\n",
    "            if array[i] - array[i-1] == 1:\n",
    "                end += 1\n",
    "            if i == len(array) - 1:\n",
    "                if end - start > maxend - maxstart:\n",
    "                    return start, end\n",
    "                else:\n",
    "                    return maxstart, maxend\n",
    "        else:\n",
    "            if end - start > maxend - maxstart:\n",
    "                maxstart = start\n",
    "                maxend = end\n",
    "            start, end = array[i], array[i]\n",
    "            \n",
    "    return maxstart, maxend\n",
    "\n",
    "largestRange(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"http://127.0.0.1:8000/api/1.0/mappings\", headers={'X-API-Key': 'btmappings'})\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-richardson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "neutral-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    data_field.data_structure\n",
       "1    data_field.data_structure\n",
       "2    data_field.data_structure\n",
       "Name: MatchingStringCatalog, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### from slugify import slugify\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class String:\n",
    "\n",
    "    def __init__(self, raw: str, idx: int) -> None:\n",
    "        self._raw = raw\n",
    "        self._idx = idx\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"String(idx={self._idx}, raw='{self._raw}', slugified='{self.slugified}')\"\n",
    "\n",
    "    @property\n",
    "    def slugified(self):\n",
    "        return slugify(self._raw, separator=\" \")\n",
    "\n",
    "\n",
    "class StringCatalog:\n",
    "\n",
    "    name = 'StringCatalog'\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._strings = set()\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self._strings)\n",
    "\n",
    "    def __repr__(self) -> None:\n",
    "        return f\"{self.name} with {len(self)} string(s)\"\n",
    "\n",
    "    def as_pandas(self) -> None:\n",
    "        return pd.Series([s._raw for s in self._strings], name=self.name)\n",
    "\n",
    "    def add_strings(self, strings: List[str]) -> None:\n",
    "        for string in strings:\n",
    "            self._strings.add(String(raw=string, idx=len(self)))\n",
    "\n",
    "    def get_string(self, idx: int = None, raw: str = None) -> String:\n",
    "        for string in self._strings:\n",
    "            if string._raw == raw or string._idx == idx:\n",
    "                return string\n",
    "\n",
    "\n",
    "class InputStringCatalog(StringCatalog):\n",
    "    name = 'InputStringCatalog'\n",
    "    \n",
    "class MatchingCandidateCatalog(StringCatalog):\n",
    "    name = 'MatchingStringCatalog'\n",
    "    \n",
    "\n",
    "input_catalog = InputStringCatalog()\n",
    "input_catalog.add_strings(strings=['entity_name.attribute_name0', 'entity_name.attribute_name1', 'entity_name.attribute_name2'])\n",
    "input_catalog.get_string(idx=2), input_catalog.get_string(raw='entity_name.attribute_name2')\n",
    "input_catalog.as_pandas()\n",
    "\n",
    "candidate_catalog = MatchingCandidateCatalog()\n",
    "candidate_catalog.add_strings(strings=['data_field.data_structure', 'data_field.data_structure', 'data_field.data_structure'])\n",
    "candidate_catalog.as_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "surprising-republic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    2\n",
       "Name: A, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(data={'A': [1,2,2], 'B': [3,3,3]})\n",
    "a[a.columns[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
